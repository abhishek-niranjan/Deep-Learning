{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 1\n",
    "\n",
    "- Do not submit your answers as images where text or an equation is expected. You might be evaluated with a zero.\n",
    "- Use mathlatex (latex notations) to type math equations\n",
    "- If at all you feel you need to add some diagram or illustration, use relative path to add them as image and make sure you include them in the zipped archive that you will be submitting in the moodle\n",
    "- Name your notebook and the zip as < rollno >_A1.zip. For example if you are roll number 13CS60R12 then submit the zip as 13CS60R12_A1.zip\n",
    "\n",
    "- The marks for the individual questions will be decided later\n",
    "- Double click on the cells where it is written \"Ans. Write your answer here.\". Markdown syntax needs to be followed while writing the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. \n",
    "We are given a dataset $(X,Y)$.  Which among the following classifiers will contain sufficient information that allows the calculation of joint probability of the features and the label in the dataset? Justify your answer for each of the classifer.\n",
    "If $X = (X_1,X_2,X_3,X_4)$ then you need to calculate $P(X_1,X_2,X_3,X_4,Y)$.\n",
    "\n",
    " - Linear Regression\n",
    " - Logisitc Regression\n",
    " - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1a](images/1.jpeg)\n",
    "![1b](images/1b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a. \n",
    "For two discrete-valued distributions $P(X), Q(X)$, K-L Divergence is defined as\n",
    " \n",
    "$$ KL(P||Q) = \\sum_x P(x)log\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "where P(x) > 0. \n",
    "\n",
    "Prove the following $$ \\forall P,Q~~ KL(P||Q) \\geq 0 $$ and\n",
    "$$ KL(P||Q) ~ = ~ 0 ~ iff ~ P ~ = ~ Q$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2a1](images/2a1.jpeg)\n",
    "![2a2](images/2a2.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Q2b.\n",
    "The KL-Divergence between two conditional distributions $P(X|Y),Q(X|Y)$ is\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$\n",
    "\n",
    "\n",
    "Prove the following chain rule for KL Divergence:\n",
    "$$ KL(P(X,Y)||Q(X,Y) = KL(P(X)||Q(X)) + KL(P(Y|X)||Q(Y|X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![2b](images/2b.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "What is the role of the activation function in a neural network? What would happen if you just used the identify function as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function serves as a threshold, alternatively called classification or a partition also called \"Space Folding\". It essentially divides the original space into typically 2 partitions. Activation functions are usually introduced as requiring to be a non-linear function. This might be too restrictive, rather recently piecewise linear functions (i.e. ReLU) have been shown to work just as well in practice.\n",
    "Activation functions for the hidden units are needed to introduce nonlinearity into the network. Without nonlinearity, hidden units would not make nets more powerful than just plain perceptrons (which do not have any hidden units, just input and output units). The reason is that a linear function of linear functions is again a linear function. However, it is the nonlinearity (i.e, the capability to represent nonlinear functions) that makes multilayer networks so powerful. Almost any nonlinear function does the job, except for polynomials. For backpropagation learning, the activation function must be differentiable, and it helps if the function is bounded.\n",
    "The purpose of an activation function in a Deep Learning context (i.e. multiple layers) is to ensure that the representation in the input space is mapped to a different space in the output. In all cases a similarity function between the input and the weights are performed by a neural network. This can be an inner product, a correlation function or a convolution function. In all cases it is a measure of similarity between the learned weights and the input. This is then followed by a activation function that performs a threshold on the calculated similarity measure. In its most general sense, a neural network layer performs a projection that is followed by a selection. Activation functions are decision functions, the ideal decision function is the heaviside step function. But this is not differentiable hence more smoother versions such as the sigmoid function have been used merely because of the fact that they are differentiable which makes them ideal for gradient based optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "Assume a friend of yours recently got diogonised for a rare disease and it is given that the testing methods for this disease are correct 99 percent of the time. You did some googling and found that the chances of the disease to occur randomly in the general population is only one of every 10,000 people. What are the chances that your friend actually have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4a](images/4a.jpeg)\n",
    "![4b](images/4b.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Q5. \n",
    "How exactly is the training of structured perceptron different from that of a perceptron? Explain how we can solve argmax problem for sequences in the context of a structured perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a structured perceptron, we find the sequence that has the most probability to generate the output.To obtain the post-probable sequence which can be an exhaustive search or a special optimisation algorithm (like Viterbi), instead of backpropagation. For argmax problem, Viterbi is used instead of doing an exhaustive search. It uses MAP interface to predict the most probable path to the current state. Viterbi finds this argmax efficiently (assuming each context Cj contains only the previous m tags; i.e. assuming an m-gram tagger). The structured perceptron training algorithm is given as follows: Inputs: Training examples(xk, yk) Initialization: λ = 0 Algorithm: For l = 1 to L, k = 1 to n Use Viterbi to get zk = argmax zλ · Φ(xk, z) If zk is unequal to yk then λ=λ + Φ(xk, yk)− Φ(xk, zk) Output: weights λ Assume n tagged sentences for training Initialise weights to zero Do this: L passes over the training data For each tagged sentence in the training data, find the highest scoring tag sequence using the current weights If the highest scoring tag sequence matches the gold, move to next sentence. If not,  not, for each feature in the gold but not in the output, add 1 to its weight; for each feature in the output but not in the gold, take 1 from its weight Return weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "## Q6. \n",
    "We already know differentiation in the context of univariate real valued functions (input $\\in \\mathbb{R}$, output $\\in \\mathbb{R}$). For e.g $\\frac{d}{dx}(x^2 +x) = 2x + 1$.\n",
    "\n",
    "Now we define differentiation in the context of matrices and vectors. Consider a function $f(\\mathbf{x}) = \\mathbf{y}$, where $\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T \\in \\mathbb{R}^n$ and $\\mathbf{y} = (y_1, y_2, \\dots, y_m)^T \\in \\mathbb{R}^m$ are vectors. We define the derivative of $f(\\mathbf{x})$ wrt $\\mathbf{x}$ as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\[2mm]\n",
    "\t\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\t\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\\\\\n",
    "\t\\end{bmatrix}\n",
    "\\text{ or } \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_{ij} = \\frac{\\partial y_i}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{x}$ is a scalar (denote by $x$) then we define the derivative as a vector of elementwise derivatives given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial x} f(x)\\right]_i = \\frac{\\partial y_i}{\\partial x}\n",
    "$$\n",
    "Similarly, if $\\mathbf{y}$ is a scalar (denote by $y$) then we define the derivative as,\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_i = \\frac{\\partial y}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{y}$ is a scalar (denote by $y$) and $\\mathbf{x}$ is a matrix (denote by $X$), then we define the derivative as a matrix given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial X} f(X)\\right]_{ij} = \\frac{\\partial y}{\\partial X_{ij}}\n",
    "$$\n",
    "\n",
    "Given that $A, X\\in \\mathbb{R}^{a\\times b}$ and $v, x\\in \\mathbb{R}^{b}$, show the following:\n",
    " - $\\frac{\\partial}{\\partial x} v^T x = \\frac{\\partial}{\\partial x} x^T v = v$\n",
    " - $\\frac{\\partial}{\\partial x} Ax = A$\n",
    " - $\\frac{\\partial}{\\partial x} x^TAx = Ax + A^Tx$\n",
    "\n",
    "Using the above results, show the following result (which is actually the solution to least squares linear regression)\n",
    "$$\n",
    "    \\underset{w}{\\arg \\min} \\| Xw-Y\\|_2^2 = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "(Hint: $\\|v\\|_2^2 = v^Tv$. Write the above norm in this form, differentiate and equate to zero.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![6a](images/6a.jpeg)\n",
    "![6b](images/6b.jpeg)\n",
    "![6c](images/6c.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Q7a \n",
    "You are given a Neural Network model which claims to detect between Huskies and Wolves. You are also shown the predictions of the model on 10 held-out images. \n",
    "\n",
    "The results show amongs 10 images (5 each from both the classes), it mis-predicts the 2 cases (1 each from each of the class - 6th and 9th images) of the 10 images.\n",
    "\n",
    "\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "This particular question does not look for the exact answer and rather this question wants to test your thinking and reasoning capacity. So try to come up with multiple possible explanations. <i>The subsequent question will show what exactly was the neural network learning, and hence it is implied that we expect different answers from what is given below. So attempt the next question only after finishing this question</i> \n",
    "\n",
    "![Wolf or Huskies](images/7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Let's take Wolf as 1. Husky as 0. Then, \n",
    "   True Positive(TP) = 4.\n",
    "   \n",
    "   True Negatives(TN) = 4.\n",
    "   \n",
    "   False Positives(FP) = 1.\n",
    "   \n",
    "   False Negatives(FN) = 1.\n",
    "   \n",
    "   Hence, Recall = 4/5 = 0.8\n",
    "   \n",
    "   Precision = 4/5 = 0.8\n",
    "   \n",
    "   Recall can be considered good, but the precision of 0.8 is never considered decent. Precision means the amount of misclassifications, and a model having 20% misclassifications is not considered good.\n",
    "   \n",
    "   \n",
    "(ii). The system might be learning from the [Pixel Color, Key Descriptors]\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 7b) \n",
    "Given below are the previously shown images along with its corresponding reconstructed portions obtained from the neural network.\n",
    "\n",
    "\n",
    "The non-gray parts on the reconstructed images are the parts of the image that the neural network thinks are the most important in making the predictions. With the new evidence (assuming you answered previous question without looking into this), please reanswer the above question\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "![Double click and remove the exclamation mark inside the parenthesis to see the image](images/7b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i). Since, There are multiple type of key descriptors in an image(eg. Corners, Edges, Key points, etc.), We can see that the model is not looking for Corners, Edges but might be looking for the key points(pixels) which have very low color gradient values with the neighbouring pixels, and according to this block of pixels it is predicting the animal type. I somehow, do not trust the model as the key point can be still imporvised. \n",
    "\n",
    "\n",
    "(ii). The system might be learning from the feature vector [Pixel Color Gradient, Key Descriptors, Largest Block of Pixels having very small Pixel Color Gradient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment was originally conducted as part of the work. [\"Why Should I Trust You?\"](http://www.arxiv.org/abs/1602.04938): Explaining the Predictions of Any Classifier. \n",
    "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. In: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining \n",
    "    \n",
    "[Marco](https://homes.cs.washington.edu/~marcotcr/) was kindful enough to share the images used in the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
