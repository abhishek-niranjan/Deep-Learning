{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(X,W,b):\n",
    "    \"\"\"\n",
    "    Computes H = sigmoid(X . W + b) corresponding to the hidden unit\n",
    "    activations of a one-hidden-layer MLP classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        Batch of examples of shape (batch_size, num_vis)\n",
    "    W : numpy.ndarray\n",
    "        Weight matrix of shape (num_vis, num_hid)\n",
    "    b : numpy.ndarray\n",
    "        Bias vector of shape (num_hid, )\n",
    "    \"\"\"\n",
    "    preActivation = np.dot(X, W) + b\n",
    "    return (1.0)/(1.0 + np.exp(-preActivation))\n",
    "\n",
    "################# TEST SIGMOID #############################\n",
    "\n",
    "# H = np.random.randint(5, size=[2,4])\n",
    "# print H\n",
    "# V = np.random.randint(5, size=[4,10])\n",
    "# print V\n",
    "# d = np.random.randint(5, size=10)\n",
    "# print d\n",
    "# sigmoid(H,V,d)\n",
    "    \n",
    "\n",
    "\n",
    "def softmax(H, V, d):\n",
    "    \"\"\"\n",
    "    Computes Y = softmax(H . V + d) corresponding to the output probabilities\n",
    "    of a one-hidden-layer MLP classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    H : numpy.ndarray\n",
    "        Batch of hidden unit activations of shape (batch_size, num_hid)\n",
    "    V : numpy.ndarray\n",
    "        Weight matrix of shape (num_hid, num_classes)\n",
    "    d : numpy.ndarray\n",
    "        Bias vector of shape (num_classes, )\n",
    "    \"\"\"\n",
    "    postActivation = np.dot(H,V) + d\n",
    "    expVector = np.exp(postActivation)\n",
    "    return expVector/(np.sum(expVector, axis=1)[:,np.newaxis])\n",
    "\n",
    "######################## TEST SOFTMAX #################################\n",
    "\n",
    "# a = np.random.rand(10)\n",
    "# print a\n",
    "# # a = a[:,np.newaxis]\n",
    "# # print a\n",
    "# print softmax(a)\n",
    "\n",
    "\n",
    "\n",
    "def loss(Y, T):\n",
    "    \"\"\"\n",
    "    Computes the binary cross-entropy loss of an MLP classifier\n",
    "    Parameters\n",
    "    ----------*/\n",
    "    Y : numpy.ndarray\n",
    "        Batch of output probabilities of shape (batch_size, num_classes)\n",
    "    T : numpy.ndarray\n",
    "        Batch of one-hot encoded targets of shape (batch_size, num_classes) / Target Function\n",
    "    \"\"\"\n",
    "    lossFunction = -(T*np.log(Y)).sum(axis=1).mean(axis=0)\n",
    "    return lossFunction\n",
    "\n",
    "############### TEST LOSS #######################\n",
    "\n",
    "# V = np.random.rand(4,10)\n",
    "# print V\n",
    "# k = np.random.randint(9, size=4)\n",
    "# print k\n",
    "# d = np.zeros((k.size,10), dtype=np.int)\n",
    "# d[np.arange(k.size), k] = 1\n",
    "# print d\n",
    "\n",
    "# loss(V,d)\n",
    "\n",
    "def forward(X,W,b,V,d):\n",
    "    \"\"\"\n",
    "    Does the forward-prop on an MLP classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        Batch of examples of shape (batch_size, num_vis)\n",
    "    W : numpy.ndarray\n",
    "        Weight matrix of shape (num_vis, num_hid)\n",
    "    b : numpy.ndarray\n",
    "        Bias vector of shape (num_hid, )\n",
    "    V : numpy.ndarray\n",
    "        Weight matrix of shape (num_hid, num_classes)\n",
    "    d : numpy.ndarray\n",
    "        Bias vector of shape (num_classes, )\n",
    "    Returns\n",
    "    -------\n",
    "    H : numpy.ndarray\n",
    "        Batch of activations in hidden layer shape (batch_size, num_hid)\n",
    "    Y : numpy.ndarray\n",
    "        Batch of probability vectors of shape (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    H = sigmoid(X, W, b)\n",
    "    Y = softmax(H, V, d)\n",
    "    return H, Y\n",
    "\n",
    "def calculateGradient(H, Y, T, V, X):\n",
    "    VGrad = np.dot(H.T, Y-T)/H.shape[0]\n",
    "    dGrad = (Y-T).mean(axis=0)\n",
    "#     print Y-T\n",
    "#     print dGrad\n",
    "#     print \"dGrad.shape\", dGrad.shape\n",
    "    WGrad = np.dot(X.T, np.dot(Y-T, V.T)*H*(1-H))/X.shape[0]\n",
    "    bGrad = (np.dot((Y-T), V.T)) * H.T* ((1 - H))\n",
    "#     print bGrad\n",
    "#     print bGrad.shape\n",
    "#     k = bGrad.mean(axis=0)\n",
    "#     print \"k\",k.shape\n",
    "    bGrad = bGrad.mean(axis=0)\n",
    "    return [VGrad, dGrad, WGrad, bGrad]\n",
    "\n",
    "\n",
    "################ TEST CalculateGradient #######################\n",
    "\n",
    "# X = np.random.randint(5, size=[2,4])\n",
    "# print X, \"X\"\n",
    "# H = np.random.randint(5, size=[2,4])\n",
    "# print \"H\", H\n",
    "# Y = np.random.randint(5, size=[2,10])\n",
    "# print \"Y\", Y\n",
    "# k = np.random.randint(9, size=2)\n",
    "# print \"k\", k\n",
    "# T = np.zeros((k.size,10), dtype=np.int)\n",
    "# T[np.arange(k.size), k] = 1\n",
    "# print \"T\", T\n",
    "# V = np.random.randint(5, size=[4,10])\n",
    "# print \"V\", V\n",
    "# print calculateGradient(H,Y,T)\n",
    "\n",
    "\n",
    "\n",
    "def updateWeight(V, d, W, b, learningRate, gradList):\n",
    "    V -= learningRate*gradList[0]\n",
    "    d -= learningRate*gradList[1]\n",
    "    W -= learningRate*gradList[2]\n",
    "    b -= learningRate*gradList[3]\n",
    "    return [V, d, W, b]\n",
    "\n",
    "def train(W, b, V, d, dataX,dataY):\n",
    "        \n",
    "        ############# Model Parameters #########################\n",
    "        \n",
    "        epochs = 5\n",
    "        batchSize = 100\n",
    "        count = 0\n",
    "        learningRate = 0.001\n",
    "        noClasses = 10\n",
    "                 \n",
    "        \n",
    "        ################### Model Training #########################\n",
    "        \n",
    "        for i in range(epochs) :\n",
    "            for j in range(int(len(dataX)/batchSize)) :\n",
    "                X = dataX[j*batchSize:(j+1)*batchSize]\n",
    "                T = dataY[j*batchSize:(j+1)*batchSize]\n",
    "                k = np.zeros((T.size, noClasses))\n",
    "                k[np.arange(T.size), T] = 1\n",
    "                T = k \n",
    "                [H, Y] = forward(X, W, b, V, d)\n",
    "                lossValue = loss(Y, T)\n",
    "                if count%100 == 0 :\n",
    "                    print lossValue\n",
    "                gradList = calculateGradient(H, Y, T, V, X)\n",
    "#                 for i in range(len(gradList)):\n",
    "#                     print gradList[i].shape\n",
    "                [V, d, W, b] = updateWeight(V, d, W, b, learningRate, gradList)\n",
    "                count+=1\n",
    "        np.save('Weights_V.npy',V)\n",
    "        np.save('Weights_d.npy', d)\n",
    "        np.save('Weights_W.npy', W)\n",
    "        np.save('Weights_b.npy', b)\n",
    "        \n",
    "\n",
    "def test(dataX, dataY):\n",
    "    V = np.load('Weights_V.npy')\n",
    "    d = np.load('Weights_d.npy')\n",
    "    W = np.load('Weights_W.npy')\n",
    "    b = np.load('Weights_b.npy')\n",
    "    count = 0\n",
    "    for i in range(len(dataX)):\n",
    "        X = dataX[i:i+1]\n",
    "        T = dataY[i:i+1]\n",
    "        [H, Y] = forward(X, W, b, V, d)\n",
    "#         print Y\n",
    "        lossValue = loss(Y, T)\n",
    "        Y = np.argmax(Y)\n",
    "#         print Y, T\n",
    "        if(Y==T):\n",
    "            count += 1\n",
    "    print \"Test Accuracy is %g\" %(float(count)/float(len(dataX)))\n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "2.29900811801\n",
      "2.24923968625\n",
      "2.19489057859\n",
      "2.12336392485\n",
      "2.1077343834\n",
      "1.99095251202\n",
      "1.96877861964\n",
      "1.86964168479\n",
      "1.79456629531\n",
      "1.69922157778\n",
      "1.7462904951\n",
      "1.55626116284\n",
      "1.59851679416\n",
      "1.47636235866\n",
      "1.39921169953\n",
      "1.33134307119\n",
      "1.43115829682\n",
      "1.21357952579\n",
      "1.30578957159\n",
      "1.17538900736\n",
      "1.11809814781\n",
      "1.07117198336\n",
      "1.18924914518\n",
      "0.983416435732\n",
      "1.09589442226\n",
      "0.959975336296\n",
      "0.916682589315\n",
      "0.88603174645\n",
      "1.00654939831\n",
      "0.824250140446\n",
      "Test Accuracy is 0.8804\n"
     ]
    }
   ],
   "source": [
    "def load_mnist():\n",
    "    data_dir = './Data/'\n",
    "#     print os.path.join(data_dir, 'train-images.idx3-ubyte')\n",
    "    fd = open(os.path.join(data_dir, 'train-images.idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trX = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 'train-labels.idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trY = loaded[8:].reshape((60000)).astype(np.int)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-images.idx3-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teX = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n",
    "\n",
    "    fd = open(os.path.join(data_dir, 't10k-labels.idx1-ubyte'))\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teY = loaded[8:].reshape((10000)).astype(np.int)\n",
    "\n",
    "    trY = np.asarray(trY)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    perm = np.random.permutation(trY.shape[0])\n",
    "    trX = trX[perm]\n",
    "    trY = trY[perm]\n",
    "\n",
    "    perm = np.random.permutation(teY.shape[0])\n",
    "    teX = teX[perm]\n",
    "    teY = teY[perm]\n",
    "\n",
    "    return trX, trY, teX, teY\n",
    "\n",
    "def main():\n",
    "    trainX, trainY, testX, testY = load_mnist()\n",
    "    trainX = np.reshape(trainX,[-1,784])\n",
    "    testX = np.reshape(testX,[-1,784])\n",
    "    print trainX.shape\n",
    "    print trainY.shape\n",
    "    #print testX.shape\n",
    "    \n",
    "    ############## Model Weight Vectors #####################\n",
    "        \n",
    "    inputSize = 28*28\n",
    "    hiddenSize = 100\n",
    "    noClasses = 10\n",
    "\n",
    "    ############## Training Data ###########################3\n",
    "    \n",
    "    W = -0.01*np.random.randn(inputSize,hiddenSize)\n",
    "    b = np.zeros([hiddenSize])\n",
    "    V = -0.01*np.random.randn(hiddenSize, noClasses)\n",
    "    d = np.zeros([noClasses])\n",
    "    \n",
    "    train(W, b, V, d, trainX, trainY)\n",
    "    test(testX, testY)   \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
